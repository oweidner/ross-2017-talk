<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">

  <title>Seastar HPC Telemetry Framework | Ole Weidner | University of Edinburgh</title>

  <meta name="description" content="Towards Data-Centric HPC Platforms">
  <meta name="author" content="Ole Weidner">

  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

  <link rel="stylesheet" href="css/reveal.css">
  <link rel="stylesheet" href="css/theme/moon.css" id="theme">

  <!-- Code syntax highlighting -->
  <link rel="stylesheet" href="lib/css/zenburn.css">

  <!-- Printing and PDF exports -->
  <script>
  var link = document.createElement( 'link' );
  link.rel = 'stylesheet';
  link.type = 'text/css';
  link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
  document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>

  <!--[if lt IE 9]>
  <script src="lib/js/html5shiv.js"></script>
  <![endif]-->
</head>

<body>

  <div class="reveal">

    <div class="slides">

      <!-- SLIDE 01 -->
      <!------------------------------------------------------------------------------------------->
      <section>
        <!-- <h1><font style="background-color: #ca1e3b">&nbsp; SeaSTAR &nbsp;</font></h1> -->
        <h2 style="background-color: #ca1e3b">
          Towards a Unified Telemetry Service Framework for HPC Environments</h2>
          <br/>

          <table>
            <tr>
              <td align="center"> <small>
                <!-- <img class="plain" src="images/university_of_edinburgh.png" style="max-height:40px"> <br/> -->
                Ole Weidner <br/>
                School of Informatics <br/>
                University of Edinburgh <br/>
                <i><a href="mailto:ole.weidner@ed.ac.uk">ole.weidner@ed.ac.uk</a></i>
              </small> </td>
              <td align="center"> <small>
                <!-- <img class="plain" src="images/university_of_st_andrews.png" style="max-height:40px"> <br/> -->
                Adam Barker <br/>
                School of Computer Science <br/>
                University of St Andrews <br/>
                <i><a href="mailto:adam.barker@st-andrews.ac.uk">adam.barker@st-andrews.ac.uk</a></i>
              </small> </td>
              <td align="center"> <small>
                <!-- <img class="plain" src="images/university_of_edinburgh.png" style="max-height:40px"> <br/> -->
                Malcolm Atkinson <br/>
                School of Informatics <br/>
                University of Edinburgh <br/>
                <i><a href="mailto:malcolm.atkinson@ed.ac.uk">malcolm.atkinson@ed.ac.uk</a></i>
              </small> </td>
            </table>

            <br/><br/>

            <h4>International Workshop on Runtime and Operating Systems for Supercomputers</h4>
            <h4> Washington, D.C., USA, June 27, 2017 </h4>

          </section>
          <!------------------------------------------------------------------------------------------->

          <!-- SLIDE XX -->
          <!------------------------------------------------------------------------------------------->
          <!-- <section>
          <h2>About</h2>
          <ul>
          <li>Big-data engineer at LEGO System A/S in Denmark</li>
          <li>Part-time PhD student at the University of Edinburgh </li>
          <li>Previously: research associate at Louisiana State University and Rutgers University, USA</li>
          <li>Have been involved for more then 10 years in many <i>second generation</i> HPC application projects</li>
          <ul>
          <li>SAGA: The Simple API for Grid Applications</li>
          <li>RADICAL-Pilot workload management system</li>
          <li>EnsembleMD molecular dynamics toolkit</li>
          <li>ATLAS PanDA workload management system</li>
        </ul>
      </ul>
      <br><br>
    </section> -->
    <!------------------------------------------------------------------------------------------->


    <!-- SLIDE XX -->
    <!------------------------------------------------------------------------------------------->
    <section>
      <h2>Outline</h2>
      <ol>
        <li>Application Challenges and Motivation</li>
        <li>Telemetry as HPC Platform Service</li>
        <li>Context Graph Model</li>
        <li>Interaction and Interface</li>
        <li>Prototype</li>
        <li>Discussion</li>

      </ol>
    </section>
    <!------------------------------------------------------------------------------------------->

    <!-- SLIDE XX -->
    <!------------------------------------------------------------------------------------------->

    <section>

      <h2>Definition</h2>

      <p><i>
        HPC Telemetry Data
      </i></p>

      <p>
        Any data that describes the state of an HPC platform and the state of
        the process-based representation of the applications running on it.
      </p>

    </section>


    <section>

      <section> <!-- SLIDE XX -->

        <h1>1</h1>
        <h2>Application Challenges &amp; Motivation</h2>

        <aside class="notes">
          I think it is important to understand where our motivation
          comes from, because what we propose here is probably not
          equaly important or even relavent to the entire HPC community.

          I want to start with some small anecdote from the
          every day life (or better my every day life) as an application
          developer.

          A couple of years ago I was working as a research programmer
          at Louisiana State University and I ended up doing a lot of
          work in bioinformatics and molecular dynamics simulation,
          helping scientists to get their small scale simulations and
          proof-of-concept implementations on the big HPC systems at a
          much larger scale.

          Both in bioinformatics and MD we have a plethora of software tools
          and applications. NAMD probably one of the more popular ones
          because it won the Gordon Bell AND Sidney Fernbach Award.

          It is quite intersting that even for the same tools, runtime
          characteristics, parallel scalability,  can vary dramatically,
          depending on the model parameters or the input data.

          Some simulations would run for hours on thousands of CPU cores, while
          others would only run for a few minutes or even seconds and would
          scale pretty badly, if at all.

          For the latter category usually use someting we call a PILOT JOB
          framework, basically an overlay scheduling menchanism which allows
          us to run 10 or 100 of thousands of small, short-running jobs in
          one large batch job.


        </aside>

      </section>

      <section> <!-- SLIDE XX -->

        <h2>A Normal Day at the Office</h2>

        <p>Strange runtime distribution of homogeneous tasks</p>

        <img class="plain" src="images/s1_1.png" style="max-height:450px"> <br/>

        <aside class="notes">
          This is the runtime distribution of a number of simulation
          tasks. Those tasks have a fairly balanced I/O to compute
          tatio: each individual tasks starts with downloading a specimen
          from a remote HTTP endpoint, loads it together with a fairly large
          reference dataset from the cluster filesystem into memory and
          starts running some sort of a matching algorithm on it.

          Most of them finish in the expected time, about XX minutes, however,
          there are two clusters of tasks that don't seem to behave nicely.

          It's a small number, but in practice this has lead to some
          problems because the users of the simulation toolkit, which
          genereally don't have a CS background saw this step, which is
          part of a multi-stage pipeline, failing and simply started to adjust
          the wall-time limit of the batch job to the worst case, to somthing
          like one hour. This obviously lead to a lot of hollow utilization.

          So we started to look into this, but could not reproduce it
          on our own systems. On the system the scientists were using
          the outliers would also not show up consistentyl.
          So we started to instrument the framework.

        </aside>

      </section>

      <!-- <section>

      <h2>Observations</h2>

      <p>Outliers seem to have network and filesystem I/O issues</p>

      <img class="plain" src="images/s1_2.png" style="max-height:450px"> <br/>

      <aside class="notes">

      <ul>
      <li>
      So we would instrument the tasks to revel a bit more debug
      information, i.e., how much time is spent doing what.
    </li>
    <li>
    We also modified the  pilot job framework to submit one extra
    process per cluster node to record data about OS-level process
    metrics, or what we call TELEMETRY DATA.
  </li>
  <li>
  We did a few runs and it quickly became clear that the
  struggeling tasks seem to have either network or filsystem I/O
  issues.
</li>

</ul>

</aside>

</section> -->

<section>

  <h2>Finding the Culprint</h2>

  <ul>
    <li>Added logging to the application to understand where time is spent</li>
    <ul>
      <li>Some tasks spent 10x longer downloading input dataset</li>
      <li>A faulty edge switch caused external connectivity issues on some nodes</li>
    </ul>
    <li>Introduced helper tasks that collect process-level metrics</li>
    <ul>
      <li>Some tasks spent a hughe amount of time in IO Wait</li>
      <li>A strange problem with Lustre caused slow filesystem I/O on a small set of nodes</li>
    </ul>
  </ul>

  <aside class="notes">
    <ul>

      <li>
        <strong>Putting it into the framework</strong>
        Assuming that these things could happen anytime and
        anywhere, we started to put the mechanisms we have developed
        into our application framework so that faulty nodes can
        be blacklisted automatically.
      </li>

    </ul>
  </aside>

</section>

<section> <!-- SLIDE XX -->

  <h2>Another Intresting Case</h2>

  <p>
    Again, an unexpected runtime distribution of supposedly
    homogeneous simulation tasks
  </p>

  <img class="plain" src="images/s2_1.png" style="max-height:450px"> <br/>

  <aside class="notes">

    <ul>
      <li>
        Notes
      </li>
    </ul>

  </aside>

</section>

<!-- <section>

  <h2>Observations</h2>

  <p>
    Outliers seem to run out of memory and crash
  </p>

  <img class="plain" src="images/s2_2.png" style="max-height:450px"> <br/>

  <aside class="notes">
    <ul>
      <li>
        <strong>Again, difficult to reproduce</strong>
      </li>
      <li>
        <strong>Again, we instrument</strong>
      </li>
    </ul>

  </aside>

</section> -->

<section> <!-- SLIDE XX -->

  <h2>Finding the Culprint</h2>

  <ul>
    <li>Used the same instrumentation strategy</li>
    <ul>
      <li>
        Outlier tasks run out of memory and stall
      </li>
    <li>
      Specific structural properties of the input data would cause the
      algorithm to take a different trajectory
    </li>

  </ul>
  </ul>


  <aside class="notes">

    <ul>
      <ul>
        <strong>Put it into the framework</strong>
      </ul>
    </ul>

  </aside>

</section>

<section>
  <h2>Consequences</h2>

  <ul>

    <li>
      We encountered unexpected "dynamic behavior", both on the system as well
     as on the application side
   </li>

   <li>
     Knowing that these are no edge cases, we started making
     our "debugging" approach a more vital part of the application framework:
     <ul>
       <li>Collecting process- and OS-level information during all runs</li>
     </ul>
   </li>

  <li>Applying simple adaptive strategies to mitigate issues at runtime:</li>
  <ul>
    <li>
      Blacklist 'weird' nodes
    </li>
    <li>
      Reducing the task-packing (preempt other tasks on the node) when
      memory usage exceeds threshold
    </li>
  </ul>
</ul>

</section>

<section> <!-- SLIDE XX -->

  <h2>Experience &amp; Lessons Learned</h2>
  <ul>
    <li>Instrumetation requires a lot of effort</li>
    <li>Collecting and analysing data (at scale) is non-trivial</li>
    <li>Interpreting and feeding the data to the application is difficult</li>
    <li>Existing tooling is sparse and mostly geard toward post-mortem,
      parallel code debugging
    <li>
      Without knowing and understanding the platform "anatomy" and context, data
      can be difficult to interpret, e.g., what is considered "poor" I/O,
      what is the spatial layout of processes across nodes?
    </li>

    </ul>

    <aside class="notes">
      <ul>
        <li>Notes</li>
      </ul>
    </aside>

  </section>

  <section> <!-- SLIDE XX -->

    <h2>Experience &amp; Lessons Learned <i>cont.</i></h2>

    <ul>
      <li>Application-specific instrumentation is wide spread technique to mitigate heterogeneity, dynamic behavior, etc.</li>
      <li>Adressing the issue is expensive, but ignoring it can be expensive, too:</li>
    </ul>

    <img class="plain" src="images/epcc_data.png"> <br/>

    <aside class="notes">
      <ul>
        <li>
          <strong>Wide spread: </strong> surveyed autonomous and
          dynamic application literature and it seems that
          application-specific solutions are common.
        </li>
        <li>
          Often details about the how are missing entirely
        </li>
      </ul>
    </aside>

  </section>

</section>
<!------------------------------------------------------------------------------------------->


<!-- SLIDE XX -->
<!------------------------------------------------------------------------------------------->
<section>

  <section>
    <h1>2</h1>
    <h2>Telemetry as HPC Platform Service</h2>

    <aside class="notes">
      <ul>

        <li>
          <strong>So we started thinking...</strong>
          how would the ideal world look from our perspective.
        </li>

        <li>
          <strong>Telemetry data would be collect for us</strong>
        </li>

        <li>
          <strong>Data analysis would be done for us</strong>
        </li>

        <li>
          <strong>Control signals would be pushed to us</strong>
        </li>

      </ul>
    </aside>

  </section>

  <section>
    <h2>Status Quo: Application-Driven </h2>

    <p>Application-level collection and processing of telemtry data can cause a lot of overhead.</p>

    <img class="plain" src="images/data_collection_as_platform_service.png" style="max-height:450px"> <br/>

  </section>

  <section>
    <h2>Platform Service Approach</h2>

    <p> Telemetry service takes over data collection and provides data access and higher-level functions to applications</p>

    <img class="plain" src="images/data_collection_as_platform_service_2.png" style="max-height:450px"> <br/>

    <aside class="notes">
      <ul>
        <li>
          <strong>Proposal we made in our paper</strong>
          And in a way what I am now bringing to this workshop

        </li>
      </ul>
    </aside>

  </section>

  <section>
    <h2>Requirements</h2>

    <ul>
      <li>
        Captures the time-variant physicla anatomy and
        properties of applications
      </li>
      <li>
        Captures the time-variant anatomy and
        properties of the HPC platform
      </li>

      <li>Describes the mapping between the two (contex!)</li>

      <li>Allows for arbitrary levels of detail</li>

      <li>Provides programmatic access to the data</li>

      <li>Allows offloading data analytics, e.g. extracting trends from streams of raw data</li>

      <li>Has notifications capabilities</li>

    </ul>

  </section>

  <section>
    <h2>Requirements <i>cont.</i></h2>

    <ul>
      <li>Keeps historic data (possibly in condensed form)</li>
      <li>Is deployable at scale (think exascale!)</li>
      <li>Consistent across platforms</li>
    </ul>

  </section>

</section>

<!-- SLIDE XX -->
<!------------------------------------------------------------------------------------------->
<section>

  <section>
    <h1>3</h1>

    <h2>Context Graph Model</h2>

    <aside class="notes">
      <ul>

        <li>
          <strong>Data difficult to use without the context</strong>
          So simply collecting metrics and time series of data points
          would not be sufficient.
        </li>

        <li>
          <strong>We want to capture the structure or anatomy</strong>
        </li>


      </ul>
    </aside>

  </section>

  <section>
    <img class="plain" src="images/graph_model.png"> <br/>

  </section>


  <section>
    <h2>Graph-Based Model</h2>

    <ul>
      <li>Provides the context in which time-series can be embedded</li>
      <li>We use attributed graphs to describe entities and their relationships</li>
      <li>Graphs provide a intuitive way to model arbitrary levels of complexity</li>
      <li>A single <b>context graph</b> (<i>CG</i>) captures the connections between the
        <b>platform anatomy</b> (sub-)graph (<i>PAG</i>) and the <b>application anatomy</b>
        (sub-)graphs (<i>AAG</i>)</li>
      </ul>

    </section>

    <section>
      <h2>Spatial-Temporal Dynamics</h2>

      <ul>
        <li>Anatomy and structure of platform and applications is not static:</li>

        <ul>
          <li>Application process start and stop</li>
          <li>Nodes appear and disappear</li>
          <li>Hardware (e.g., GPUs or FPGAs) is added</li>

          <li>...</li>
        </ul>

        <li>All nodes and edges have timestamps that qualify their existence</li>

        <li>To get a snapshot of the platform and applications at a
          specific point in time, the graph can be queried for a specific
          time or time range
        </li>

      </ul>

    </section>

    <!-- <section>
      <h2>Telemetry Data as Graph Attributes</h2>
      TODO
    </section> -->

  </section>

  <!-- SLIDE XX -->
  <!------------------------------------------------------------------------------------------->
  <section>

    <section>
      <h1>4</h1>
      <h2>Interaction and Interface</h2>
    </section>

    <section>
      <h2>User- / Application-Facing API</h2>

      <ul>
        <li>Language-Agnostic HTTP/REST API allows to: </li>
        <ul>
          <li>Explore / traverse the context graph</li>
          <li>Register simple "server-side" "derived metrics" functions</li>
          <li>Define and register call-backs (Websockets)</li>
          <li>GraphQL for complex graph queries</li>
        </ul>
      </ul>

      <pre><code data-trim data-noescape>
{
 process(id: 1) {
   siblings {
     processes {
       cpu_iowait
       memory_uses
     }
   }
 }
}
      </code></pre>


    </section>

    <!-- <section>
      <h2>Graph Queries</h2>


    </section> -->

    <!-- <section>
      <h2>Notification and Call-Backs</h2>

      Describe how they are registerd and
      http://graphql.org

      <pre><code data-trim data-noescape>
        ...
        don't forget to add example from paper
        ...
      </code></pre>

    </section> -->

  </section>


  <!------------------------------------------------------------------------------------------->

  <section>

    <section>
      <h1>5</h1>
      <h2>Prototype</h2>
    </section>

    <section>
      <h2>System Components</h2>

      <img class="plain" src="images/implementation.png" style="max-height:450px"> <br/>

    </section>

  </section>



  <!-- SLIDE XX -->
  <!------------------------------------------------------------------------------------------->
  <section>

    <section>
      <h1>6</h1>

      <h2>Discussion</h2>

      <p>This is how we envision an ideal system from the application developer's
        / user's perspective</p>

      <!-- <i>"Try to do as little work as possible."</i> -->

    </section>

  </section>
  <!------------------------------------------------------------------------------------------->

  <!------------------------------------------------------------------------------------------->


  <section style="text-align: left;">
    <h1>THANK YOU</h1>
    <p>
      Slides available online: <br>
      <a style="background-color: #ca1e3b" href="https://oweidner.github.io/ross-2017-talk">
        https://oweidner.github.io/ross-2017-talk</a>
      </p>
    </section>

  </div>

</div>

<script src="lib/js/head.min.js"></script>
<script src="js/reveal.js"></script>

<script>

// Full list of configuration options available at:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({
  controls: true,
  progress: true,
  history: true,
  center: true,

  // parallaxBackgroundImage: "./images/parallax.jpg",

  // Parallax background size
  // parallaxBackgroundSize: "3000px 1694px", // - currently only pixels are supported (don't use % or auto)

  // Number of pixels to move the parallax background per slide
  // - Calculated automatically unless specified
  // - Set to 0 to disable movement along an axis
  // parallaxBackgroundHorizontal: 20,
  // parallaxBackgroundVertical: 50,

  slideNumber: true,
  slideNumber: 'c/t',

  transition: 'slide', // none/fade/slide/convex/concave/zoom

  // Optional reveal.js plugins
  dependencies: [
    { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
    { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
    { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
    { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
    { src: 'plugin/zoom-js/zoom.js', async: true },
    { src: 'plugin/notes/notes.js', async: true }
  ]
});

</script>

</body>
</html>
